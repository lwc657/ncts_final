{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 城市用水預測\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 組員\n",
    "廖偉欽"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 介紹\n",
    "水資源是國家追求永續發展的項目之一，也是永續發展的重要關鍵要素。近年來在極端氣候下，水資源的取得環境越來越嚴峻，更顯其珍貴。在有限的水資源下，了解未來需求的變化即為重要課題。  \n",
    "對於自來水事業而言，售水量的預測與其營運息息相關，也是未來供水規劃的基礎。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 實作方法\n",
    "本專案使用包含NN的機器學習演算法作為自來水事業售水量預測方法。\n",
    "## 專案排程\n",
    "第一周： 資料來源 (5/3~5/10)  \n",
    "第二周： 資料分析 + 預處理 (5/10~5/17)  \n",
    "第三周： 模型建構 + 進度報告 (5/17~5/24)  \n",
    "第四周： 模型評估及修正  (5/24~5/31)  \n",
    "第五周： 期末專題  (5/31~6/7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 目前進度\n",
    "資料讀入、處理、各種驗算法。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load main3.py\n",
    "#%% packages\n",
    "\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "#import matplotlib.dates as md\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import Lasso, LassoCV, LassoLarsCV, LassoLarsIC\n",
    "from sklearn.linear_model import RidgeCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import xgboost\n",
    "import datetime as dt\n",
    "import statsmodels.api as sm\n",
    "import itertools\n",
    "plt.style.use('seaborn-white')\n",
    "from statsmodels.tsa.arima_model import ARIMA\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import Sequential\n",
    "print(tf.__version__)\n",
    "print(keras.__version__)\n",
    "\n",
    "np.random.seed(1337)\n",
    "PYTHONHASHSEED = 0\n",
    "tf.random.set_random_seed(1337)\n",
    "tf.set_random_seed(1337)\n",
    "\n",
    "featSelect = 1\n",
    "mName = 'xgboost'\n",
    "\n",
    "#%% load data\n",
    "finalDataDf = pd.read_csv('data/MyData12.csv')\n",
    "finalData = finalDataDf.values\n",
    "X = finalData[:,:-1]\n",
    "y = finalData[:,-1]\n",
    "\n",
    "\n",
    "#%% feature normalization  and preprocess\n",
    "testRate = 0.2\n",
    "numTrain = int(len(X) * (1-testRate))\n",
    "numTest = 1\n",
    "\n",
    "m = X[:numTrain].mean(axis=0)\n",
    "s = X[:numTrain].std(axis=0)\n",
    "\n",
    "X = ( X - m ) / s\n",
    "X[:,0:2] = 0\n",
    "realX = X\n",
    "#X_train = ( X_train - m ) / s\n",
    "#X_test = ( X_test - m ) / s\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = testRate, shuffle = False)\n",
    "\n",
    "numTest = len(y_test)\n",
    "#numTrain = 1 - numTest\n",
    "\n",
    "vec_date = np.vectorize(dt.date)\n",
    "\n",
    "t = vec_date(finalData[:,0], finalData[:,1], 1)\n",
    "#t_test = vec_date(X_test[:, 0], X_test[:, 1], 1)\n",
    "t_test = vec_date(finalData[numTrain:, 0], finalData[numTrain:, 1], 1)\n",
    "allFig = plt.figure()\n",
    "allFigax = allFig.add_subplot(111)\n",
    "allFigax.plot_date(t, y, '-', label='actual')\n",
    "allFigax.legend(loc='upper right')\n",
    "allFigax.set_xlabel(\"time\")\n",
    "allFigax.set_ylabel('volume(m3)')\n",
    "\n",
    "#%% feature selection\n",
    "if featSelect == 1 and mName != 'random forest' and mName != 'xgboost':\n",
    "    lassoIC = LassoLarsIC()\n",
    "    lassoIC.fit(X_train, y_train)\n",
    "    coef = lassoIC.coef_\n",
    "    X = X[:,coef!=0]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "    \n",
    "    coef = np.append(coef,0)\n",
    "    print(finalDataDf.columns[coef!=0])\n",
    "    \n",
    "    #di = {'feature':finalDataDf.columns[coef!=0], 'coef':coef[coef!=0]}\n",
    "    #selectedFeature = pd.DataFrame(data=di)\n",
    "    \n",
    "    selectedFeature = pd.DataFrame(data=coef[coef!=0], index=finalDataDf.columns[coef!=0], columns=['coef'])\n",
    "    selectedFeature.sort_values(by=['coef'],ascending=0).to_csv(\"featureCoef.csv\")\n",
    "    \n",
    "#%% time series\n",
    "        \n",
    "realY = y\n",
    "\n",
    "y = pd.DataFrame(y_train)\n",
    "y.index = pd.DatetimeIndex(t[:numTrain])\n",
    "decomposition = sm.tsa.seasonal_decompose(y[0], model='additive')\n",
    "\n",
    "fig = decomposition.plot()\n",
    "plt.xlabel(\"time\")\n",
    "\n",
    "\n",
    "p = d = q = range(0, 2)\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]\n",
    "\n",
    "Param = []\n",
    "Param_seasonal = []\n",
    "AIC = []\n",
    "    \n",
    "    #%% SARIMAX\n",
    "    \n",
    "if mName == 'sarima':\n",
    "    for param in pdq:\n",
    "        for param_seasonal in seasonal_pdq:\n",
    "            try:\n",
    "                \n",
    "                mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                                order=param,\n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "                results = mod.fit()\n",
    "                Param.append(param)\n",
    "                Param_seasonal.append(param_seasonal)\n",
    "                AIC.append(results.aic)\n",
    "                print('ARIMA{}x{}12 - AIC:{}'.format(param, param_seasonal, results.aic))\n",
    "            except:\n",
    "                continue\n",
    "            \n",
    "    minAIC = AIC.index(min(AIC))\n",
    "    \n",
    "    mod = sm.tsa.statespace.SARIMAX(y,\n",
    "                                    order=Param[minAIC],\n",
    "                                    seasonal_order=Param_seasonal[minAIC],\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "    results = mod.fit()\n",
    "    print(results.summary().tables[1])\n",
    "    \n",
    "    results.plot_diagnostics(figsize=(16, 8))\n",
    "    plt.show()\n",
    "    \n",
    "    pred_uc = results.get_forecast(steps=numTest)\n",
    "    pred_ci = pred_uc.conf_int()\n",
    "    #ax = yttrain.plot(label='observed', figsize=(14, 7))\n",
    "    ax = y.plot(label='observed', figsize=(14, 7))\n",
    "    pred_uc.predicted_mean.plot(ax=ax, label='Forecast')\n",
    "    ax.fill_between(pred_ci.index,\n",
    "                    pred_ci.iloc[:, 0],\n",
    "                    pred_ci.iloc[:, 1], color='k', alpha=.25)\n",
    "    ax.set_xlabel('Date')\n",
    "    ax.set_ylabel('Furniture Sales')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    predictions = pred_uc.predicted_mean.values\n",
    "    \n",
    "    #mse = ((predictions - y_test) ** 2).mean()\n",
    "    Mape = np.mean(abs(predictions - y_test)/y_test)\n",
    "    #\n",
    "    #print('The Mean Squared Error of our forecasts is {}'.format(round(mse, 2)))\n",
    "   \n",
    "    \n",
    "    #%% ARIMA\n",
    "elif mName == 'arima':\n",
    "    Param = []\n",
    "    \n",
    "    AIC = []     \n",
    "    for param in pdq:\n",
    "        \n",
    "        try:\n",
    "            \n",
    "            mod = ARIMA(y, order=param)\n",
    "            results = mod.fit()\n",
    "            Param.append(param)\n",
    "            \n",
    "            AIC.append(results.aic)\n",
    "            print('ARIMA{} - AIC:{}'.format(param, results.aic))\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    minAIC = AIC.index(min(AIC))\n",
    "    \n",
    "    mod = ARIMA(y,order=Param[minAIC])\n",
    "    results = mod.fit()\n",
    "    print(results.summary().tables[1])    \n",
    "    \n",
    "    predictions = results.forecast(steps=numTest)[0]\n",
    "    #predictions = results.predict(start=dt.datetime(X_test[0,0], X_test[0,1], 1),\n",
    "    #                end=dt.datetime(X_test[-1,0], X_test[-1,1], 1))\n",
    "    Mape = np.mean(abs(predictions - y_test)/y_test)\n",
    "    \n",
    "    #%% test plot\n",
    "else:\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(211)\n",
    "    plt.plot_date(t_test, y_test,'-', label='actual')\n",
    "    plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.ylabel('water supply(t)')\n",
    "    plt.title('Water Supply Prediction')\n",
    "    \n",
    "    predAll = []\n",
    "    \n",
    "    ax2 = fig.add_subplot(212)\n",
    "    plt.plot_date(t_test, y_test-y_test,'-', label='actual')\n",
    "    plt.grid(axis=\"y\")\n",
    "    plt.ylabel('error(t)')\n",
    "    plt.xlabel('time')\n",
    "    \n",
    "    def draw_prediction(predictions, y_test, mName):\n",
    "        \n",
    "        res = predictions - y_test\n",
    "        ax.plot_date(t_test, predictions,'-', label=mName)\n",
    "        ax.legend(loc='best')\n",
    "        ax2.plot_date(t_test, res, '-', label=mName)\n",
    "        ax2.legend(loc='best')\n",
    "        \n",
    "        allFigax.plot_date(t_test, predictions,'-', label=mName)\n",
    "        allFigax.legend(loc='best')\n",
    "        # np.mean(abs(y_train_pred - y_train))/np.mean(y_train)\n",
    "        print(np.mean(abs(res/y_test)))\n",
    "        # print(np.mean(abs(res))/np.mean(y_test))\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "#%% y normalization\n",
    "\n",
    "    \n",
    "    mY = y_train.mean()\n",
    "    sY = y_train.std()\n",
    "    y_train = ( y_train - mY ) / sY\n",
    "    # y_test = ( y_test - mY ) / sY\n",
    "    \n",
    "    \n",
    "    \n",
    "    #%% lasso regression\n",
    "    \"\"\"\n",
    "    lassocv.alplha_ is different from which in R\n",
    "    \"\"\"\n",
    "if mName=='lasso':   \n",
    "    mName = 'lasso'\n",
    "    lassocv = LassoLarsIC()\n",
    "    lassocv.fit(X_train, y_train)\n",
    "    y_train_pred = lassocv.predict(X_train)\n",
    "    predictions = lassocv.predict(X_test)\n",
    "    predictions = predictions * sY + mY\n",
    "    #predAll = np.append(predAll,predictions).reshape([-1,1])\n",
    "    coef = lassocv.coef_\n",
    "    lassocv.alpha_\n",
    "    \n",
    "    draw_prediction(predictions, y_test, mName)\n",
    "    \n",
    "    ## lassocv = LassoCV(random_state=0, eps=1e-9, cv=10, n_alphas=100)\n",
    "    #lassocv = LassoLarsCV()\n",
    "    #lassocv.fit(X_train, y_train)\n",
    "    #y_train_pred = lassocv.predict(X_train)\n",
    "    #predictions = lassocv.predict(X_test)\n",
    "    #np.mean(abs(y_train_pred - y_train))/np.mean(y_train)\n",
    "    #np.mean(abs(predictions - y_test))/np.mean(y_test)\n",
    "    #lassocv.coef_\n",
    "    #lassocv.alpha_\n",
    "    #plt.plot_date(t, predictions,'-', label='lassocv')\n",
    "    #plt.legend(loc='upper right')\n",
    "    \n",
    "    #%% Ridge regression\n",
    "elif mName=='ridge':\n",
    "    mName = 'ridge'\n",
    "    lassocv = RidgeCV()\n",
    "    lassocv.fit(X_train, y_train)\n",
    "    coef = lassocv.coef_\n",
    "    \n",
    "    y_train_pred = lassocv.predict(X_train)\n",
    "    predictions = lassocv.predict(X_test)\n",
    "    predictions = predictions * sY + mY\n",
    "    #predAll = np.append(predAll,predictions.reshape([-1,1]), axis=1)\n",
    "    \n",
    "    coef = lassocv.coef_\n",
    "    lassocv.alpha_\n",
    "    \n",
    "    draw_prediction(predictions, y_test, mName)\n",
    "    \n",
    "    # ax.plot_date(t_test, predictions,'-', label=mName)\n",
    "    # ax.legend(loc='best')\n",
    "    # res = predictions - y_test\n",
    "    # ax2.plot_date(t_test, res, '-', label=mName)\n",
    "    # ax2.legend(loc='best')\n",
    "    # np.mean(abs(y_train_pred - y_train))/np.mean(y_train)\n",
    "    # np.mean(abs(predictions - y_test))/np.mean(y_test)\n",
    "    \n",
    "    #%% Random Forest\n",
    "elif mName=='random forest':\n",
    "    X = realX\n",
    "    y = realY\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "    y_train = ( y_train - mY ) / sY\n",
    "    mName = 'random forest'\n",
    "    regr = RandomForestRegressor(random_state=1337)\n",
    "    regr.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    \n",
    "    temp = regr.feature_importances_\n",
    "    temp.sort()\n",
    "    coef = regr.feature_importances_\n",
    "    #coef = np.append(coef,0)\n",
    "    \n",
    "    if featSelect == 1:\n",
    "        \n",
    "        X = X[:,coef>=temp[-3]]\n",
    "        y = realY\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "        coef = np.append(coef,0)\n",
    "        y_train = ( y_train - mY ) / sY\n",
    "        \n",
    "        \n",
    "        selectedFeature = pd.DataFrame(data=coef[coef>=temp[-3]], index=finalDataDf.columns[coef>=temp[-3]], columns=['coef'])\n",
    "        selectedFeature.sort_values(by=['coef'],ascending=0).to_csv(\"featureCoef.csv\")\n",
    "        print(selectedFeature)\n",
    "        regr = RandomForestRegressor(random_state=1337)\n",
    "        regr.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_train_pred = regr.predict(X_train)\n",
    "    predictions = regr.predict(X_test)\n",
    "    predictions = predictions * sY + mY\n",
    "    #predAll = np.append(predAll,predictions.reshape([-1,1]), axis=1)\n",
    "    \n",
    "    draw_prediction(predictions, y_test, mName)\n",
    "    \n",
    "    # ax.plot_date(t_test, predictions, '-', label=mName)\n",
    "    # ax.legend(loc='best')\n",
    "    \n",
    "    # res = predictions - y_test\n",
    "    # ax2.plot_date(t_test, res, '-', label=mName)\n",
    "    # ax2.legend(loc='best')\n",
    "    # np.mean(abs(y_train_pred - y_train))/np.mean(y_train)\n",
    "    # np.mean(abs(predictions - y_test))/np.mean(y_test)\n",
    "    \n",
    "    #%% XGBoost\n",
    "elif mName=='xgboost':\n",
    "    X = realX\n",
    "    y = realY\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "    y_train = ( y_train - mY ) / sY\n",
    "    mName = 'xgboost'\n",
    "    xgb = xgboost.XGBRegressor(seed=1337)\n",
    "    xgb.fit(X_train, y_train, verbose=True)\n",
    "    \n",
    "    temp = xgb.feature_importances_\n",
    "    temp.sort()\n",
    "    coef = xgb.feature_importances_\n",
    "    \n",
    "    if featSelect == 1:\n",
    "    \n",
    "        X = X[:,coef>=temp[-3]]\n",
    "        y = realY\n",
    "        coef = np.append(coef,0)\n",
    "        selectedFeature = pd.DataFrame(data=coef[coef>=temp[-3]], index=finalDataDf.columns[coef>=temp[-3]], columns=['coef'])\n",
    "        selectedFeature.sort_values(by=['coef'],ascending=0).to_csv(\"featureCoef.csv\")\n",
    "        print(selectedFeature)\n",
    "        \n",
    "        # 4 feature selected rather than 3\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, shuffle = False)\n",
    "        \n",
    "        y_train = ( y_train - mY ) / sY\n",
    "        \n",
    "        xgb = xgboost.XGBRegressor()\n",
    "        xgb.fit(X_train, y_train)\n",
    "    \n",
    "    \n",
    "    y_train_pred = xgb.predict(X_train)\n",
    "    predictions = xgb.predict(X_test)\n",
    "    predictions = predictions * sY + mY\n",
    "    #predAll = np.append(predAll,predictions.reshape([-1,1]), axis=1)\n",
    "    \n",
    "    draw_prediction(predictions, y_test, mName)\n",
    "    \n",
    "    #%% y normalization\n",
    "    \n",
    "    #mY = y_train.mean()\n",
    "    #sY = y_train.std()\n",
    "    #y_train = ( y_train - mY ) / sY\n",
    "    #y_test = ( y_test - mY ) / sY\n",
    "    \n",
    "    #%% NN\n",
    "elif mName=='FFNN':\n",
    "    mName = 'FFNN'\n",
    "    val_loss = []\n",
    "    model_rec = []\n",
    "    history_rec = []\n",
    "    \n",
    "    for i in range(10):\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(Dense(units=3, activation='relu', input_dim=X.shape[1]))\n",
    "        #model.add(Dense(units=3, activation='relu', input_dim=X.shape[1]))\n",
    "        #model.add(Dense(units=3, activation='relu', input_dim=X.shape[1]))\n",
    "        #model.add(Dense(units=3, activation='relu', input_dim=X.shape[1]))\n",
    "        model.add(Dense(units=3, activation='relu'))\n",
    "        model.add(Dense(units=3, activation='relu'))\n",
    "        model.add(Dense(units=3, activation='relu'))\n",
    "        model.add(Dense(units=1))\n",
    "        model.summary()\n",
    "        \n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "    #    model.save_weights('model.h5')\n",
    "        \n",
    "        \n",
    "        model_rec.append(model)\n",
    "        \n",
    "        patience = 200\n",
    "        es=keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         mode='min',\n",
    "                                         patience=patience)\n",
    "        history = model.fit(X_train, y_train, epochs=20000, validation_split=0.2, callbacks=[es])\n",
    "        \n",
    "        history_rec.append(history)\n",
    "        val_loss.append(history.history['val_loss'][-1])\n",
    "    \n",
    "       \n",
    "    model = model_rec[val_loss.index(min(val_loss))]\n",
    "    history = history_rec[val_loss.index(min(val_loss))]\n",
    "    \n",
    "    #epochs = epoch_rec[val_loss.index(min(val_loss))] - patience\n",
    "    epochs = len(history.epoch) - patience\n",
    "    #model.load_weights('model.h5')\n",
    "    model.fit(X_train, y_train, epochs=epochs)\n",
    "    \n",
    "    \n",
    "    predictions = model.predict(X_test) * sY + mY\n",
    "    predictions = predictions.reshape(-1)\n",
    "    #predAll = np.append(predAll,predictions.reshape([-1,1]), axis=1)\n",
    "    res = predictions - y_test\n",
    "    \n",
    "    draw_prediction(predictions, y_test, mName)\n",
    "    model.save_weights(mName + '.h5')\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    \n",
    "    \n",
    "    #%% LSTM\n",
    "    \n",
    "elif mName=='LSTM':\n",
    "    mName = 'LSTM'\n",
    "    val_loss = []\n",
    "    model_rec = []\n",
    "    history_rec = []\n",
    "    \n",
    "    X_train = X_train.reshape(-1,1,X.shape[1])\n",
    "    y_train = y_train.reshape(-1,1)\n",
    "    X_test = X_test.reshape(-1,1,X.shape[1])\n",
    "        \n",
    "    \n",
    "    for i in range(10):\n",
    "            \n",
    "        model = Sequential()\n",
    "        model.add(keras.layers.LSTM(units=3, input_shape=(1,X.shape[1]), activation='relu'))\n",
    "        model.add(Dense(units=3, activation='relu'))\n",
    "        model.add(Dense(units=3, activation='relu'))\n",
    "        model.add(Dense(units=1))\n",
    "        \n",
    "        model.summary()\n",
    "        \n",
    "        model.compile(loss='mse', optimizer='adam', metrics=['mse'])\n",
    "        \n",
    "        model.save_weights('model' + str(i) + '.h5')\n",
    "        model_rec.append(model)\n",
    "    \n",
    "        #model.fit(X_train, y_train,epochs=100)\n",
    "        \n",
    "        \n",
    "        patience = 200\n",
    "        es=keras.callbacks.EarlyStopping(monitor='val_loss',\n",
    "                                         mode='min',\n",
    "                                         patience=patience)\n",
    "        history = model.fit(X_train, y_train, epochs=20000, validation_split=0.2, callbacks=[es])\n",
    "    \n",
    "        history_rec.append(history)\n",
    "        val_loss.append(history.history['val_loss'][-1])\n",
    "    \n",
    "    model.load_weights('model' + str(val_loss.index(min(val_loss))) + '.h5')\n",
    "    #model = model_rec[val_loss.index(min(val_loss))]\n",
    "    history = history_rec[val_loss.index(min(val_loss))]\n",
    "    \n",
    "    epochs = len(history.epoch) - patience\n",
    "    #model.load_weights('model.h5')\n",
    "    \n",
    "    model.fit(X_train, y_train, epochs=epochs)\n",
    "    #X_test_comb = X_test\n",
    "    X_test_comb = np.concatenate((X_train, X_test))\n",
    "    \n",
    "    predictions = model.predict(X_test_comb) * sY + mY\n",
    "    predictions = predictions.reshape(-1)\n",
    "    predictions = predictions[-len(y_test):]\n",
    "    #predAll = np.append(predAll,predictions.reshape([-1,1]), axis=1)\n",
    "    res = predictions - y_test\n",
    "    \n",
    "    draw_prediction(predictions, y_test, mName)\n",
    "    model.save_weights(mName + '.h5')\n",
    "    \n",
    "    fig = plt.figure()\n",
    "    plt.plot(history.history['loss'], label='train')\n",
    "    plt.plot(history.history['val_loss'], label='validation')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    #%% residual figure\n",
    "else:   \n",
    "    resAll = predAll - y_test.reshape([-1,1])\n",
    "    # plt.plot_date(t_test, resAll, '-', label='XGboost')\n",
    "    plt.savefig('compare.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
